import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import warnings
import nltk
from string import punctuation
import re
from wordcloud import WordCloud
from transformers import pipeline
import os

warnings.filterwarnings('ignore')

# Init the dataset to review and rating only
# ================================================================
# 1. Data acquisition
file = pd.read_csv("Src/Shoes_Data.csv")
# ================================================================
# 2. Data processing
df = file[["reviews", "reviews_rating"]]

product_id = []
reviews = []
rates = []

for j in df.index:
    lst = [i for i in df.iloc[j].reviews.split('||')]
    lst2 = [i for i in df.iloc[j].reviews_rating.split('||')]
    for k in lst:
        product_id.append(j + 1)
        reviews.append(k)
    for l in lst2:
        rates.append(l)

df = pd.DataFrame(list(zip(product_id, reviews, rates)),
                  columns=["Product_id", 'Review', 'Review_rating'])

# Cleaning functions


def lower(text):
    return text.lower()


def remove_punctuation(text):
    return text.translate(str.maketrans('', '', punctuation))


def remove_digits(text):
    return re.sub(r'\d+', '', text)


def remove_emoji(text):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)


def remove_non_printable(text):
    text = text.encode("ascii", "ignore")
    return text.decode()


def clean_text(text):
    text = lower(text)
    text = remove_punctuation(text)
    text = remove_digits(text)
    text = remove_emoji(text)
    text = remove_non_printable(text)
    return text


# Apply the cleaning function to 'Review' column
df['clean_review'] = df['Review'].apply(clean_text)

# Save the cleaned and analyzed DataFrame to a CSV file
df.drop("Review", axis=1, inplace=True)
df.to_csv('Src/shoe_cleanData.csv', index=False)


# ================================================================
# 3. Model deployment
# Sentiment analysis using BERT model
# Use a pre-trained semantic model to evaluate the usersâ€™ emotions

# Load the sentiment analysis model
sentiment_analysis = pipeline("sentiment-analysis")

# Apply the sentiment analysis model to the cleaned reviews
df['sentiment'] = df['clean_review'].apply(lambda x: sentiment_analysis(x)[0]['label'])

# Save the updated DataFrame to a CSV file
file_path = 'Src/shoe_cleanData_semantic.csv'

if not os.path.isfile(file_path):
    df.to_csv('Src/shoe_cleanData_sentiments.csv', index=False)
    
# df.to_csv('Src/shoe_cleanData_sentiments.csv', index=False)

# ================================================================
# 4. Recommendations based on sentiment analysis
# Based on the extracted user emotions and your analysis, give some concrete recommendations for the sellers.

# Group the DataFrame by product_id
grouped_df = df.groupby('Product_id')

# Calculate the average sentiment score for each product
product_sentiments = grouped_df['sentiment'].value_counts(normalize=True).unstack()

# Identify the products with the most positive sentiment
most_positive_products = product_sentiments.idxmax(axis=0)

# Identify the products with the most negative sentiment
most_negative_products = product_sentiments.idxmin(axis=0)

# Print recommendations for sellers
for product_id, sentiment in most_positive_products.items():
    print(f"Product {product_id}: Positive sentiment. Customers are satisfied.")

for product_id, sentiment in most_negative_products.items():
    print(f"Product {product_id}: Negative sentiment. Customers are not satisfied. Improvement needed.")
